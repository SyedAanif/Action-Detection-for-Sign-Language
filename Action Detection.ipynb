{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2a27edf",
   "metadata": {},
   "source": [
    "In this notebook we will try to implement **Sign/Action Language Detection** using *Python*, *Tensorflow* and *MediaPipe Hoslistic Pipelines*.\n",
    "\n",
    "Actually here we will be focussing on sequence of actions rather than a single frame to detect the sign. So the model or architecture of choice would be **Long Short Term Memory(LSTM)**.\n",
    "\n",
    "[MediaPipe Holistic](https://github.com/google/mediapipe/blob/master/docs/solutions/holistic.md)\n",
    "\n",
    "[LSTM](https://en.wikipedia.org/wiki/Long_short-term_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6b9b93",
   "metadata": {},
   "source": [
    "# Install Required Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c00a32d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.4.1\n",
      "  Using cached tensorflow-2.4.1-cp38-cp38-win_amd64.whl (370.7 MB)\n",
      "Requirement already satisfied: tensorflow-gpu==2.4.1 in c:\\users\\syeda\\anaconda3\\lib\\site-packages (2.4.1)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\syeda\\anaconda3\\lib\\site-packages (4.5.5.64)\n",
      "Requirement already satisfied: mediapipe in c:\\users\\syeda\\anaconda3\\lib\\site-packages (0.8.11)\n",
      "Requirement already satisfied: google-pasta~=0.2 in c:\\users\\syeda\\anaconda3\\lib\\site-packages (from tensorflow==2.4.1) (0.2.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in c:\\users\\syeda\\anaconda3\\lib\\site-packages (from tensorflow==2.4.1) (1.12.1)\n",
      "Requirement already satisfied: gast==0.3.3 in c:\\users\\syeda\\anaconda3\\lib\\site-packages (from tensorflow==2.4.1) (0.3.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in c:\\users\\syeda\\anaconda3\\lib\\site-packages (from tensorflow==2.4.1) (2.4.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in c:\\users\\syeda\\anaconda3\\lib\\site-packages (from tensorflow==2.4.1) (1.1.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in c:\\users\\syeda\\anaconda3\\lib\\site-packages (from tensorflow==2.4.1) (1.6.3)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\syeda\\anaconda3\\lib\\site-packages (from tensorflow==2.4.1) (3.19.1)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in c:\\users\\syeda\\anaconda3\\lib\\site-packages (from tensorflow==2.4.1) (3.3.0)\n",
      "Requirement already satisfied: wheel~=0.35 in c:\\users\\syeda\\anaconda3\\lib\\site-packages (from tensorflow==2.4.1) (0.37.1)\n",
      "Requirement already satisfied: grpcio~=1.32.0 in c:\\users\\syeda\\anaconda3\\lib\\site-packages (from tensorflow==2.4.1) (1.32.0)\n",
      "Requirement already satisfied: absl-py~=0.10 in c:\\users\\syeda\\anaconda3\\lib\\site-packages (from tensorflow==2.4.1) (0.15.0)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in c:\\users\\syeda\\anaconda3\\lib\\site-packages (from tensorflow==2.4.1) (1.12)\n",
      "Requirement already satisfied: h5py~=2.10.0 in c:\\users\\syeda\\anaconda3\\lib\\site-packages (from tensorflow==2.4.1) (2.10.0)\n",
      "Requirement already satisfied: six~=1.15.0 in c:\\users\\syeda\\anaconda3\\lib\\site-packages (from tensorflow==2.4.1) (1.15.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in c:\\users\\syeda\\anaconda3\\lib\\site-packages (from tensorflow==2.4.1) (3.7.4.3)\n",
      "Requirement already satisfied: tensorboard~=2.4 in c:\\users\\syeda\\anaconda3\\lib\\site-packages (from tensorflow==2.4.1) (2.7.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in c:\\users\\syeda\\anaconda3\\lib\\site-packages (from tensorflow==2.4.1) (1.1.2)\n",
      "Requirement already satisfied: numpy~=1.19.2 in c:\\users\\syeda\\anaconda3\\lib\\site-packages (from tensorflow==2.4.1) (1.19.5)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\syeda\\anaconda3\\lib\\site-packages (from mediapipe) (3.6.2)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\users\\syeda\\anaconda3\\lib\\site-packages (from mediapipe) (4.7.0.72)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\syeda\\anaconda3\\lib\\site-packages (from mediapipe) (22.1.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\syeda\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow==2.4.1) (65.6.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\syeda\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow==2.4.1) (2.28.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\syeda\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow==2.4.1) (0.6.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\syeda\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow==2.4.1) (0.4.6)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\syeda\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow==2.4.1) (2.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\syeda\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow==2.4.1) (3.3.6)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\syeda\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow==2.4.1) (2.3.3)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\syeda\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow==2.4.1) (1.8.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\syeda\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (9.3.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\syeda\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (1.4.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\syeda\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (0.11.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\syeda\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (1.0.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\syeda\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (22.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\syeda\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (4.25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\syeda\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\syeda\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (3.0.9)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\syeda\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\syeda\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\syeda\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1) (4.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\syeda\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.1) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\syeda\\anaconda3\\lib\\site-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4.1) (4.11.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\syeda\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.1) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\syeda\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.1) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\syeda\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.1) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\syeda\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.1) (1.26.13)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\syeda\\anaconda3\\lib\\site-packages (from werkzeug>=0.11.15->tensorboard~=2.4->tensorflow==2.4.1) (2.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\syeda\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4.1) (3.8.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\syeda\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\syeda\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.1) (3.1.1)\n",
      "Installing collected packages: tensorflow\n",
      "Successfully installed tensorflow-2.4.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\syeda\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\syeda\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\syeda\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\syeda\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\syeda\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\syeda\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\syeda\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.4.1 tensorflow-gpu==2.4.1 opencv-python mediapipe\n",
    "# Also sklearn and matplotlib if not present already\n",
    "# !pip install sklearn matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcde00b6",
   "metadata": {},
   "source": [
    "# Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "89f18dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 # open-cv for use of webcam etc\n",
    "import numpy as np # store keyoints as arrays and mathematical manipulations\n",
    "import os # work with file-system\n",
    "\n",
    "import matplotlib.pyplot as plt # visualisation\n",
    "import time\n",
    "\n",
    "import mediapipe as mp # for extractiong keypoints from the live feed\n",
    "\n",
    "from sklearn.model_selection import train_test_split # split data\n",
    "\n",
    "# evaluating the model\n",
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical # create categories\n",
    "\n",
    "from tensorflow.keras.models import Sequential # sequential neural network\n",
    "from tensorflow.keras.layers import LSTM, Dense # for LSTM and dense layers\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint # callback for monitoring and saving model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbbdaf1",
   "metadata": {},
   "source": [
    "# Keypoints using MP Holistic\n",
    "In this we use MediaPipe Holistics to capture Keypoints from our feed. Also, use MediaPipe drawings to embed keypoints over our feed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11670a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MediaPipe Holistic Model to capture keypoints\n",
    "mp_holistic = mp.solutions.holistic\n",
    "\n",
    "# Utilities to draw keypoints over our feed\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4526d6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    # convert input image from open-cv from BGR to RGB\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # image is non-writable(for memory management)\n",
    "    image.flags.writeable = False \n",
    "    \n",
    "    # detection over passed frame(image) using MediaPipe\n",
    "    results = model.process(image)\n",
    "    \n",
    "    # image is writable\n",
    "    image.flags.writeable = True\n",
    "    \n",
    "    # convert image from RGB to BGR for open-cv\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c940403d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_detected_landmarks(image, results):\n",
    "    # draw face detections using ladnmarks and the connections for each landmark\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1),\n",
    "                             mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                             )\n",
    "    \n",
    "    # draw pose detections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=2),\n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             )\n",
    "    \n",
    "    # draw left-hand detections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=2),\n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             )\n",
    "    \n",
    "    # draw right-hand detections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=2),\n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4814b7f2",
   "metadata": {},
   "source": [
    "# Test Webcam Feed\n",
    "In this we will test the webcam feed with detected pose, face, hands using MediaPipe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a7103b60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0) # access webcam\n",
    "\n",
    "# access the MediaPipe Holistic model\n",
    "with mp_holistic.Holistic() as holistic_model:\n",
    "    while cap.isOpened(): # loop through all frames\n",
    "\n",
    "        ret, frame = cap.read() # read the frame from webcam\n",
    "\n",
    "        # Make detections using MediaPipe\n",
    "        image, results = mediapipe_detection(frame, holistic_model)\n",
    "        \n",
    "        # draw/embed landmarks over the detected image poses\n",
    "        draw_detected_landmarks(image,results)\n",
    "\n",
    "        cv2.imshow('OpenCV Feed', image) # show on screen\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'): # break from loop reading the frame\n",
    "            break\n",
    "\n",
    "    cap.release() # release webcam device\n",
    "    cv2.destroyAllWindows() # close the frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b140db62",
   "metadata": {},
   "source": [
    "# Extract Key-Points\n",
    "In this we will extract key-points from detected frames. Also, handle for the cases where we don't get any detections by blank arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "43fd3a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    # for pose-detection\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    \n",
    "    # for face-detection\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    \n",
    "    # for left-hand-detection\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    \n",
    "    # for right-hand-detection\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose, face, lh, rh])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd6367c",
   "metadata": {},
   "source": [
    "# Base Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "478b75c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path for the exported data\n",
    "DATA_PATH = os.path.join('MP_Data')\n",
    "\n",
    "# actions to be detected\n",
    "actions = np.array(['hello','thanks','iloveyou'])\n",
    "\n",
    "# no of sequences to be considered for detcetion of an action(videos)\n",
    "no_sequences = 30\n",
    "\n",
    "# each video length(frames)\n",
    "sequence_length = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "052108a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make folders for each action and each sequence\n",
    "for action in actions:\n",
    "    for sequence in range(no_sequences):\n",
    "        try:\n",
    "            os.makedirs(os.path.join(DATA_PATH,action,str(sequence)))\n",
    "        except:\n",
    "            print('Exception Occured')\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbfb10d",
   "metadata": {},
   "source": [
    "# Collect Keypoint Values(Data Collection)\n",
    "In this we will collect data for each action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d5dea9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0) # access webcam\n",
    "\n",
    "# access the MediaPipe Holistic model\n",
    "with mp_holistic.Holistic() as holistic_model:\n",
    "    \n",
    "    for action in actions: # each action\n",
    "        for sequence in range(no_sequences): # each sequence\n",
    "            for frame_num in range(sequence_length): # each frame\n",
    "\n",
    "                ret, frame = cap.read() # read the frame from webcam\n",
    "\n",
    "                # Make detections using MediaPipe\n",
    "                image, results = mediapipe_detection(frame, holistic_model)\n",
    "\n",
    "                # draw/embed landmarks over the detected image poses\n",
    "                draw_detected_landmarks(image,results)\n",
    "                \n",
    "                # Apply Wait Logic\n",
    "                if frame_num == 0: \n",
    "                    cv2.putText(image, 'STARTING COLLECTION', (120,200), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n",
    "                    cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    \n",
    "                    # show on screen\n",
    "                    cv2.imshow('OpenCV Feed', image)\n",
    "                    cv2.waitKey(2000) # wait for 2 sec\n",
    "                else: \n",
    "                    cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "                    # show on screen\n",
    "                    cv2.imshow('OpenCV Feed', image)\n",
    "                    \n",
    "                keypoints = extract_keypoints(results) # Extract Keypoints\n",
    "                \n",
    "                # save a numpy array in each action and sequence folder\n",
    "                npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num))\n",
    "                np.save(npy_path, keypoints)\n",
    "\n",
    "                if cv2.waitKey(10) & 0xFF == ord('q'): # break from loop reading the frame\n",
    "                    break\n",
    "\n",
    "    cap.release() # release webcam device\n",
    "    cv2.destroyAllWindows() # close the frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021c11a5",
   "metadata": {},
   "source": [
    "# Data Pre-Processing\n",
    "In this we will do a `train-test split` of the data. Also, create labels and features for the deep neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c7fbb43a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hello': 0, 'thanks': 1, 'iloveyou': 2}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create label map\n",
    "label_map = {label:num for num, label in enumerate(actions)}\n",
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d52834b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences, labels = [], [] # feature data(X), labels(Y)\n",
    "\n",
    "for action in actions: # each action\n",
    "    for sequence in range(no_sequences): # each sequence\n",
    "        window = [] # all frames for a sequence\n",
    "        for frame_num in range(sequence_length): # each frame\n",
    "            # load numpy based frames\n",
    "            res = np.load(os.path.join(DATA_PATH, action, str(sequence), \"{}.npy\".format(frame_num)))\n",
    "            window.append(res)\n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "197c790a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90, 30, 1662)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# features\n",
    "X = np.array(sequences)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "d336d6aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90, 3)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# labels\n",
    "y = to_categorical(labels).astype(int)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "30852f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60eb54b",
   "metadata": {},
   "source": [
    "# Build and Train LSTM Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "668f60d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join('Logs') # setup log directory\n",
    "tb_callback = TensorBoard(log_dir=log_dir) # monitor using TensorBoard callback\n",
    "\n",
    "# NOTE: use tensorboard extension or command line\n",
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir <<log-directory>>\n",
    "\n",
    "# tensorboard --logdir=<<log-directory>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "633573b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the best model using model checkpoint callback\n",
    "model_checkpoint = ModelCheckpoint('action_net.h5', \n",
    "                                    save_best_only=True, \n",
    "                                    monitor='categorical_accuracy', \n",
    "                                    mode='max', \n",
    "                                    verbose=1\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "d71e43a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the neural network\n",
    "model = Sequential() # initialise the sequential model\n",
    "\n",
    "# have multi-layer LSTM(stacked)\n",
    "model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(30,1662)))\n",
    "model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "\n",
    "# use Dense layers(fully-connected layers)\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "\n",
    "# multi-class classification layer for actions\n",
    "model.add(Dense(actions.shape[0], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "361fc7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model with Adam optimizer, categorical cross entropy\n",
    "model.compile(optimizer='Adam', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['categorical_accuracy']\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "878e36ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_39 (LSTM)               (None, 30, 64)            442112    \n",
      "_________________________________________________________________\n",
      "lstm_40 (LSTM)               (None, 30, 128)           98816     \n",
      "_________________________________________________________________\n",
      "lstm_41 (LSTM)               (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 596,675\n",
      "Trainable params: 596,675\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# check model architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "cf5f4886",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  fit the model\n",
    "model.fit(X_train, y_train, \n",
    "          epochs=2000, \n",
    "          callbacks=[tb_callback, model_checkpoint]\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14b13b8",
   "metadata": {},
   "source": [
    "# Load Saved Model, Make Predictions, Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "af618832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load saved weights to the model\n",
    "model.load_weights('action_net.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "1afabfc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.9792609e-03, 3.2631186e-05, 9.9698812e-01],\n",
       "       [9.9938071e-01, 2.1581777e-12, 6.1929988e-04],\n",
       "       [9.9194777e-11, 9.9999976e-01, 2.8206767e-07],\n",
       "       [9.9930382e-01, 1.8642418e-14, 6.9621176e-04],\n",
       "       [9.9897718e-01, 1.0028432e-11, 1.0227843e-03],\n",
       "       [1.0539658e-09, 9.9997330e-01, 2.6754913e-05],\n",
       "       [4.7263089e-11, 1.0000000e+00, 1.4124115e-08],\n",
       "       [4.7726557e-02, 3.8103924e-05, 9.5223540e-01],\n",
       "       [9.2308837e-01, 4.0486885e-09, 7.6911665e-02]], dtype=float32)"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict on test data\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "38a9e304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'iloveyou'"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[np.argmax(y_pred[0])] # predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "15109969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'iloveyou'"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[np.argmax(y_test[0])] # actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "72f65789",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.argmax(y_test, axis=1).tolist()\n",
    "y_hat = np.argmax(y_pred, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "92e31829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 0, 1, 0, 0, 1, 1, 2, 0]"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "3e8ed5e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 0, 1, 0, 0, 1, 1, 2, 0]"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "5f979b59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[5, 0],\n",
       "        [0, 4]],\n",
       "\n",
       "       [[6, 0],\n",
       "        [0, 3]],\n",
       "\n",
       "       [[7, 0],\n",
       "        [0, 2]]], dtype=int64)"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the confusion matrix\n",
    "multilabel_confusion_matrix(y_true, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "97ce2e30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy\n",
    "accuracy_score(y_true,y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac02201",
   "metadata": {},
   "source": [
    "# Test on Live Feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "106279ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [(245,117,16), (117,245,16), (16,117,245)]\n",
    "def prob_viz(res, actions, input_frame, colors):\n",
    "    output_frame = input_frame.copy()\n",
    "    for num, prob in enumerate(res):\n",
    "        cv2.rectangle(output_frame, (0,60+num*40), (int(prob*100), 90+num*40), colors[num], -1)\n",
    "        cv2.putText(output_frame, actions[num], (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "    return output_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "cecabf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = [] # sequence of 30 frames\n",
    "sentence = [] # text\n",
    "predictions = []\n",
    "threshold = 0.4 # confidence threshold\n",
    "\n",
    "cap = cv2.VideoCapture(0) # access webcam\n",
    "\n",
    "# access the MediaPipe Holistic model\n",
    "with mp_holistic.Holistic() as holistic_model:\n",
    "    while cap.isOpened(): # loop through all frames\n",
    "\n",
    "        ret, frame = cap.read() # read the frame from webcam\n",
    "\n",
    "        # Make detections using MediaPipe\n",
    "        image, results = mediapipe_detection(frame, holistic_model)\n",
    "\n",
    "        # draw/embed landmarks over the detected image poses\n",
    "        draw_detected_landmarks(image,results)\n",
    "\n",
    "        # Prediction Logic\n",
    "        keypoints = extract_keypoints(results) # Extract Keypoints\n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[-30:]\n",
    "        \n",
    "        # predict after 30 frames/sequences\n",
    "        if len(sequence) == 30:\n",
    "            res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "            # print(actions[np.argmax(res)])\n",
    "            predictions.append(np.argmax(res))\n",
    "          \n",
    "            # Visualisation logic \n",
    "            if np.unique(predictions[-10:])[0]==np.argmax(res): \n",
    "                if res[np.argmax(res)] > threshold: \n",
    "\n",
    "                    if len(sentence) > 0: \n",
    "                        if actions[np.argmax(res)] != sentence[-1]:\n",
    "                            sentence.append(actions[np.argmax(res)])\n",
    "                    else:\n",
    "                        sentence.append(actions[np.argmax(res)])\n",
    "\n",
    "            if len(sentence) > 5: \n",
    "                sentence = sentence[-5:]\n",
    "\n",
    "            # Visualisation probabilities\n",
    "            image = prob_viz(res, actions, image, colors)\n",
    "\n",
    "        cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "        cv2.putText(image, ' '.join(sentence), (3,30), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                    1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        # show on screen\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'): # break from loop reading the frame\n",
    "            break\n",
    "\n",
    "    cap.release() # release webcam device\n",
    "    cv2.destroyAllWindows() # close the frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bde7c50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
